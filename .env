# ================================
# üîß LLM Configuration
# ================================
# Ollama server URL (make sure Ollama is running here)
LLM_CHAT_URL=http://localhost:11434/api/chat

# Model name as installed/configured in Ollama
LLM_MODEL_NAME=mistral

# Whether to enable LLM calls in your app
USE_LLM=true
# Maximum tokens for LLM responses
LLM_MAX_TOKENS=4096
LLM_TEMPERATURE=0.7

# ================================
# üîç Embedding Model Configuration
# ================================
EMBED_MODEL=all-MiniLM-L6-v2

# ================================
# üì¶ Vector Index Configuration
# ================================
FAISS_INDEX_PATH=faiss_index.bin
FAISS_META_PATH=metadata.json
EMBED_DIM=384
TOP_K=3
