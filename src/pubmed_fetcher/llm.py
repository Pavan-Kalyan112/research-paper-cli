# src/pubmed_fetcher/llm.py

import os
import requests
from typing import Optional
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configuration
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "ollama").lower()
LLM_MODEL = os.getenv("LLM_MODEL_NAME", "mistral")
LLM_CHAT_URL = os.getenv("LLM_CHAT_URL", "http://localhost:11434/api/chat")
USE_LLM = os.getenv("USE_LLM", "true").strip().lower() == "true"


def summarize_with_llm(abstract: str) -> str:
    """
    Summarizes a research abstract using the configured LLM (e.g., Ollama).

    Args:
        abstract (str): The research abstract text.

    Returns:
        str: A concise summary or fallback message.
    """
    if not USE_LLM:
        return "üîá LLM summarization is disabled (USE_LLM=False)."

    if not abstract or not abstract.strip():
        return "‚ö†Ô∏è Empty abstract provided."

    try:
        response = requests.post(
            LLM_CHAT_URL.replace("/chat", "/generate"),  # Prefer /generate endpoint for summarization
            json={
                "model": LLM_MODEL,
                "prompt": f"Summarize this research abstract in clear and simple terms:\n\n{abstract.strip()}",
                "stream": False
            },
            timeout=60
        )
        response.raise_for_status()
        result = response.json().get("response", "").strip()
        return result if result else "‚ö†Ô∏è No summary generated by the model."
    except requests.exceptions.RequestException as e:
        return f"‚ö†Ô∏è Network error during summarization: {e}"
    except Exception as e:
        return f"‚ùå Unexpected summarization error: {e}"


def chat_with_llm(prompt: str, model: Optional[str] = None) -> str:
    """
    Starts a chat-like interaction with the configured LLM.

    Args:
        prompt (str): The user prompt to send.
        model (Optional[str]): Optional model override.

    Returns:
        str: The LLM response or an error message.
    """
    if not USE_LLM:
        return "üîá LLM chat is disabled (USE_LLM=False)."

    if not prompt or not prompt.strip():
        return "‚ö†Ô∏è Prompt is empty."

    model = model or LLM_MODEL

    try:
        response = requests.post(
            LLM_CHAT_URL,
            json={
                "model": model,
                "messages": [
                    {"role": "system", "content": "You are a helpful assistant specialized in explaining scientific research papers."},
                    {"role": "user", "content": prompt.strip()}
                ]
            },
            timeout=60
        )
        response.raise_for_status()
        data = response.json()

        # Support for both `message.content` and fallback to `response`
        return data.get("message", {}).get("content") or data.get("response", "‚ö†Ô∏è No response generated.")
    except requests.exceptions.RequestException as e:
        return f"‚ö†Ô∏è Network error during chat: {e}"
    except Exception as e:
        return f"‚ùå Unexpected chat error: {e}"
