import os
import requests
from dotenv import load_dotenv
from typing import Optional

# Load environment variables
load_dotenv()

# === Configuration ===
LLM_PROVIDER: str = os.getenv("LLM_PROVIDER", "ollama").lower()
LLM_MODEL: str = os.getenv("LLM_MODEL_NAME", "mistral")
LLM_CHAT_URL: str = os.getenv("LLM_CHAT_URL", "http://localhost:11434/api/chat")
USE_LLM: bool = os.getenv("USE_LLM", "true").strip().lower() == "true"
DEBUG: bool = os.getenv("LLM_DEBUG", "false").strip().lower() == "true"

# === Logging Helper ===
def log_debug(message: str) -> None:
    if DEBUG:
        print(f"[LLM DEBUG] {message}")

# === Summarization ===
def summarize_with_llm(abstract: str) -> str:
    """Summarize a given research abstract using the configured LLM."""
    if not USE_LLM:
        return "🔇 LLM summarization is disabled (USE_LLM=False)."

    abstract = abstract.strip()
    if not abstract:
        return "⚠️ Empty abstract provided."

    prompt = f"Summarize this research abstract in clear and simple terms in detail:\n\n{abstract}"
    payload = {
        "model": LLM_MODEL,
        "prompt": prompt,
        "stream": False
    }

    url = LLM_CHAT_URL.replace("/chat", "/generate")

    try:
        log_debug(f"[Summarize] Sending request to {url} with model: {LLM_MODEL}")
        response = requests.post(url, json=payload, timeout=120)
        response.raise_for_status()

        result = response.json().get("response", "").strip()
        return result or "⚠️ No summary generated by the model."

    except requests.exceptions.Timeout:
        return "⏱️ Request timed out during summarization."

    except requests.exceptions.RequestException as e:
        return f"⚠️ Network error during summarization: {e}"

    except Exception as e:
        return f"❌ Unexpected summarization error: {e}"

# === Chat with Retry ===
def chat_with_llm(prompt: str, model: Optional[str] = None) -> str:
    """Send a prompt to the LLM chat endpoint and return the response."""
    if not USE_LLM:
        return "🔇 LLM chat is disabled (USE_LLM=False)."

    prompt = prompt.strip()
    if not prompt:
        return "⚠️ Prompt is empty."

    model = model or LLM_MODEL
    payload = {
        "model": model,
        "messages": [
            {
                "role": "system",
                "content": (
                    "You are a helpful and knowledgeable assistant. "
                    "Explain the following scientific research content clearly."
                )
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
    }

    try:
        log_debug(f"[Chat] Sending chat request to {LLM_CHAT_URL} with model: {model}")
        response = requests.post(LLM_CHAT_URL, json=payload, timeout=60)
        response.raise_for_status()

        data = response.json()
        result = (
            data.get("message", {}).get("content") or
            data.get("response", "⚠️ No response generated.")
        ).strip()
        return result

    except requests.exceptions.Timeout:
        return "⏱️ Request timed out during chat."

    except requests.exceptions.RequestException as e:
        return f"⚠️ Network error during chat: {e}"

    except Exception as e:
        return f"❌ Unexpected chat error: {e}"

# === Retry Wrapper for Chat ===
def chat_with_retry(prompt: str, retries: int = 3) -> str:
    """Retry chat_with_llm() up to `retries` times if empty response or failure."""
    for attempt in range(1, retries + 1):
        log_debug(f"[Retry] Attempt {attempt}/{retries}")
        result = chat_with_llm(prompt)

        if result and not result.startswith("⚠️") and not result.startswith("❌"):
            return result

    return "❌ LLM failed to respond after multiple attempts."
